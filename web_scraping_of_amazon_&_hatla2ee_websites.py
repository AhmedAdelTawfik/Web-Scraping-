# -*- coding: utf-8 -*-
"""Web Scraping of Amazon & Hatla2ee websites.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/139w8UMYvnlX1uc93u6mvos1QtlsiIHl2
"""

#Web Scrabing Task for a website that has many pages with maultiprocessing
#i did scraping for amazon & hatla2ee

from urllib import request
import time
from toolz import compose
import os
from multiprocessing import Pool
import requests
from bs4 import BeautifulSoup
import pandas as pd

t0 = time.time()

links = []

#creating list of links that will be scrapped

for i in range(1,60):
  links.append(f"https://www.amazon.co.uk/s?i=videogames&rh=n%3A300703&fs=true&page={i}&qid=1722970689&ref=sr_pg_{i}")

#defining a function that takes the link and returns the link and its content items as a tuple

def get_items_names(link):

  #using headers & time.sleep() after requesting a webpage prevent being blocked by websites by memicing the behavior of genuine user browser

  headers = ({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36', "Accept-Language":"en-US,en;q=0.5"})

  page = requests.get(link, headers=headers) #output :<Response [200]>

  soup = BeautifulSoup(page.content, "html.parser") #output : html content

  time.sleep(3)

  tags = soup.find_all("span", attrs = {"class" : "a-size-medium a-color-base a-text-normal" } ) #output : list of tags of items but each item inside text inside the/its tag

  items_names = [] #creating empty list for appending to

  #looping the list of tags and getting out the text inside them then appending that text found in the empty list items_names
  for tag in tags:
    items_names.append(tag.text.strip())

  return link, items_names #returning the link and its content items as a tuple because multiprocessing can't ensure that the first link in links list returns its items first as 2 cores works in parralel and it is not possible to predict which core finishes first and returns the items first


#multiprocessing the map function so that execution time is reduced

with Pool(processes=2) as P:
  links_items_names = P.map(get_items_names, links)



t1 = time.time()

t = t1-t0 #calculating execution time

print(t)

df = pd.DataFrame(links_items_names) #creating a dataframe from the list of tuples

df

df[1][1]

t0 = time.time()

links = []

for i in range(1,16):
  links.append(f"https://eg.hatla2ee.com/en/car/all-prices/page/{i}")

def get_cars_names(link):

  headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}

  page = requests.get(link, headers=headers)

  soup = BeautifulSoup(page.content, "html.parser")

  container = soup.find("div", class_ = "newCarPricesItemBody")

  if container:

    tags=container.find_all("a")

    cars_names = []
    for tag in tags:
      cars_names.append(tag.text.strip())

    return link, cars_names

  else:
    return link, []

with Pool(processes=2) as P:
  links_cars_names = P.map(get_cars_names, links)


t1 = time.time()

t = t1-t0

print(t)

df = pd.DataFrame(links_cars_names)

df

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}


page = requests.get('https://eg.hatla2ee.com/en/car/all-prices/page/2', headers=headers)
soup = BeautifulSoup(page.content, "html.parser")
tags = soup.find("div", class_ = "newCarPricesItemBody").find_all("a")

for tag in tags:
  print(tag.text.strip())

